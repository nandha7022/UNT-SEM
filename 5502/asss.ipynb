{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2412d26",
   "metadata": {},
   "source": [
    "# Gradient descent with Linear Regression compared to the OLS matrix formulation of the beta coefficients using data from Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec0ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711d970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defineng the linear regression function to calculate the loss\n",
    "def linreg(beta, X, y):\n",
    "    y_pred = np.dot(X, beta)\n",
    "    loss = np.sum((y_pred - y) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d676c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining  the gradient descent function for linear regression\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    n, p = X.shape\n",
    "    # Initialize the beta and best_beta to 0; Initialize best_loss.\n",
    "    beta = np.zeros(p)\n",
    "    best_loss = float('inf')\n",
    "    best_beta = np.zeros(p)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute the gradient of the loss function at beta\n",
    "        y_pred = np.dot(X, beta)\n",
    "        gradient = 2 * np.dot(X.T, (y_pred - y))\n",
    "        \n",
    "        # Update beta\n",
    "        beta = beta - learning_rate * gradient\n",
    "        \n",
    "        # Keep track of the best seen so far loss and parameters\n",
    "        current_loss = linreg(beta, X, y)\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_beta = beta\n",
    "        \n",
    "        # Print beta and loss update within the for loop\n",
    "        if i < 10 or i > 29990:\n",
    "            print(f\"Iteration: {i}, Beta Values: {beta},\\nBest Loss: {current_loss}\")\n",
    "    \n",
    "    # Return the best beta and best loss\n",
    "    return best_beta, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034600e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the learning rate and number of iterations\n",
    "learning_rate = 0.0001\n",
    "num_iterations_linear = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a59bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Beta Values: [0.015  0.1404 0.1084],\n",
      "Best Loss: 539.0223544\n",
      "Iteration: 1, Beta Values: [0.02657888 0.25185692 0.19406028],\n",
      "Best Loss: 360.7318699583712\n",
      "Iteration: 2, Beta Values: [0.03544733 0.34038133 0.26170369],\n",
      "Best Loss: 248.7891117851275\n",
      "Iteration: 3, Beta Values: [0.04216843 0.41073554 0.31507223],\n",
      "Best Loss: 178.49778979137926\n",
      "Iteration: 4, Beta Values: [0.04718834 0.46669283 0.35713079],\n",
      "Best Loss: 134.35420528676465\n",
      "Iteration: 5, Beta Values: [0.05086055 0.51124286 0.39022852],\n",
      "Best Loss: 106.62553310358618\n",
      "Iteration: 6, Beta Values: [0.05346515 0.5467544  0.41622668],\n",
      "Best Loss: 89.20175472042988\n",
      "Iteration: 7, Beta Values: [0.05522404 0.57510428 0.43660001],\n",
      "Best Loss: 78.24715687403072\n",
      "Iteration: 8, Beta Values: [0.05631306 0.59777955 0.45251696],\n",
      "Best Loss: 71.35377691342129\n",
      "Iteration: 9, Beta Values: [0.05687151 0.61595844 0.46490333],\n",
      "Best Loss: 67.00995746756539\n",
      "Iteration: 29991, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257624\n",
      "Iteration: 29992, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257625\n",
      "Iteration: 29993, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257624\n",
      "Iteration: 29994, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.100883442576254\n",
      "Iteration: 29995, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257624\n",
      "Iteration: 29996, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257626\n",
      "Iteration: 29997, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.100883442576226\n",
      "Iteration: 29998, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257625\n",
      "Iteration: 29999, Beta Values: [-2.26303789  1.54972927 -0.2385295 ],\n",
      "Best Loss: 34.10088344257623\n"
     ]
    }
   ],
   "source": [
    "# Defining the X predictor matrix and y response vector for linear regression\n",
    "X = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 1],\n",
    "    [1, 2, 2],\n",
    "    [1, 3, 2],\n",
    "    [1, 5, 4],\n",
    "    [1, 5, 6],\n",
    "    [1, 6, 5],\n",
    "    [1, 7, 4],\n",
    "    [1, 10, 8],\n",
    "    [1, 11, 7],\n",
    "    [1, 11, 9],\n",
    "    [1, 12, 10]\n",
    "])\n",
    "\n",
    "y = np.array([1, 0, 1, 4, 3, 2, 5, 6, 9, 13, 15, 16])\n",
    "\n",
    "# calling the gradient descent function for linear regression\n",
    "best_beta, best_loss = gradient_descent(X, y, learning_rate, num_iterations_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd3769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Beta Values: [-2.26303788  1.54972927 -0.2385295 ]\n",
      "Best Loss: 34.1008834425762\n"
     ]
    }
   ],
   "source": [
    "# Print the best beta and best loss from linear regression \n",
    "print(\"Best Beta Values:\", best_beta)\n",
    "print(\"Best Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9196798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betas calculated using OLS Matrix Formulation (Linear Regression):\n",
      "[-2.2630379   1.54972927 -0.2385295 ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate betas using \"OLS Matrix Formulation\"\n",
    "Transpose_X = np.transpose(X)\n",
    "Transpose_X_X = np.dot(Transpose_X, X)\n",
    "Transpose_X_X_INV = np.linalg.inv(Transpose_X_X)\n",
    "Transpose_X_Y = np.dot(Transpose_X, y)\n",
    "betas_matrix_calc = np.dot(Transpose_X_X_INV, Transpose_X_Y)\n",
    "\n",
    "# Printing the calculated betas using the OLS Matrix Formulation\n",
    "print(\"Betas calculated using OLS Matrix Formulation (Linear Regression):\")\n",
    "print(betas_matrix_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0451aec",
   "metadata": {},
   "source": [
    "# Comparing the best beta values obtained from Homework 1, which are \n",
    "\n",
    "Beta 0: -2.263037902536326 \n",
    "Beta 1: 1.5497292675976115  \n",
    "Beta 2: -0.2385294955827928 \n",
    "In comparison to the best beta values(in homework1) from the gradient decent, there is no drastic difference  in the values\n",
    "\n",
    "The above python program shows the gradient desent for linear regression we are giving the four parameter with varible X as input data y as targeted values, learning_rate (step size for updating the weights), and num_iterations (number of iterations to run ) The program initiates by initializing the weight vector (w) and other variables necessary for tracking the best weights and the lowest loss. It proceeds by entering a loop, iteratively adjusting the weights using the gradient of the mean squared error loss function.Within each iteration, the program calculates predicted values (y_pred), computes the gradient vector, updates the weights with respect to the learning rate, and computes the current loss. It diligently maintains records of the best weights and the lowest loss encountered throughout the process. Ultimately, upon completing the specified number of iterations, the program returns the best weights and their corresponding minimal loss. The primary objective of this program is to determine the weights that minimize the mean squared error in a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d92d2",
   "metadata": {},
   "source": [
    "# This is an additional code of pseudocode of shiny serve which shows the plot of Deterministic Gradient Descent Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9063c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f(x)\n",
    "def f(x):\n",
    "    return x**4 - 10*x**2 + 2 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0aac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists\n",
    "x_values=[]\n",
    "fx_values=[]\n",
    "\n",
    "alpha = float(input(\"Enter the learning rate: \"))\n",
    "x = float(input(\"Enter the initial condition (x0): \"))\n",
    "num_iterations = int(input(\"Enter the number of iterations: \"))\n",
    "\n",
    "\n",
    "# Perform gradient descent for the function f(x)\n",
    "for i in range(1, num_iterations + 1):\n",
    "    gradient = 4 * x**3 - 20 * x - 1\n",
    "    x = x - alpha * gradient\n",
    "    x_values.append(x)\n",
    "    fx_values.append(f(x))\n",
    "\n",
    "min_fx = min(fx_values)\n",
    "optimal_x = x_values[fx_values.index(min_fx)]\n",
    "\n",
    "print(f\"Optimal x for f(x): {optimal_x}\")\n",
    "print(f\"Minimum f(x): {min_fx}\")\n",
    "\n",
    "x_range = np.linspace(-3, 3, 400)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_range, f(x_range), label=\"f(x)\")\n",
    "plt.scatter(x_values, fx_values, color='red', label=\"Gradient Descent\")\n",
    "plt.scatter(optimal_x, min_fx, color='green', marker='o', label=\"Optimal Point\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pyppeteer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4663ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nbconvert "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
